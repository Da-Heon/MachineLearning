{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LukP7ju_DRf-KgsSiZeCSC1FhKRSp2jw",
      "authorship_tag": "ABX9TyOB0WPsD7TaFgmWh0X+LQid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Da-Heon/MachineLearning/blob/main/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA(Principal Component Analysis)"
      ],
      "metadata": {
        "id": "STBnlSGpQ9o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PCA는 많은 변수(피처, 차원)를 가진 데이터에서 핵심적ㅇ니 정보를 뽑아내서 데이터를 간단한 형태로 줄이는 기법\n",
        " - 다차원 데이터를 2차원 그래프 시각화, 계산 효율성 증가, 노이즈 제거할 수 있음\n",
        "    - 현실 세계는 3차워\n",
        "    - 그림을 그리거나 사진을 찍으면 2차원 평면에 요약\n",
        "    - 어떤 각도에서 보는지에 따라 그림자(정보량)가 달라집니다\n",
        "- PCA는 수학적으로 가장 잘 보이는 각도(축)를 계산해서, 그 방향으로 데이터를 투영\n",
        "- 이 과정을 거치면, 원래보다 낮은 차원으로 줄어들면서도 핵심 정보는 최대한 유지하는 결과"
      ],
      "metadata": {
        "id": "O_XnVB9oSC8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 차원"
      ],
      "metadata": {
        "id": "Y2C6_NG7S9uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 머신러닝에서 차원(Dimension)이란, 데이터 포인트를 구성하는 각각의 독립된 특성(feature)\n",
        "> 피처 하나하나가 차원이다"
      ],
      "metadata": {
        "id": "L15jMb7YUBBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 벡터 공간 관점\n",
        "    - x=(x1, x2,....xd)\n",
        "    - 위 처럼 데이터를 표현할 때, d가 차원의 수를 의미합니다\n",
        "- 피처 관점\n",
        "    - 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비 4가지 피처\n",
        "    - 4차원 데이터\n",
        "    - 각 피처가 독립적인 정보를 담고 있기 때문에\n",
        "    - 차원이 늘어날수록 모델이 고려해야 할 정보 공간이 커집니다\n",
        "- 정보 표현 관점\n",
        "    - 차원이 많으면 데이터의 세부적인 패턴을 더 잘 표현할 수 있지만,\n",
        "    - 차원의 저주 문제가 발생\n",
        "    - 과도한 연산량과 컴퓨팅 자원이 필요 (비용 증가)"
      ],
      "metadata": {
        "id": "QZ9mbfZHUQSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 스케일러"
      ],
      "metadata": {
        "id": "VMdj9tHGVTpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Standard(!!별3개!!)\n",
        "    - 평균을 0, 분산을 1\n",
        "    - 모든 피처가 동일한 분산 단위로 참여\n",
        "        - PCA가 진짜 데이터 분산을 반영\n",
        "    - 이상치에 민감\n",
        "- Minmax\n",
        "    - 최소값을 0, 최대값을 1로 선형 변환\n",
        "    - 값으 범위를 명확히 제안\n",
        "    - 이상치에 민감\n",
        "- Robust\n",
        "    - 중앙값을 0, 1사분위~ 3사분위 범위(IQR)를 1로 변환\n",
        "    - 이상치에 거의 영향을 받지 않음\n",
        "    - IQR 밖의 분포 정보(이상치)가 완전히 무시\n",
        "\n",
        "- PCA는 스케일링 필수"
      ],
      "metadata": {
        "id": "8YI25oOGVXgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 차원 축소 절차"
      ],
      "metadata": {
        "id": "zw0ToARgWrLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 평균을 0으로 만들기\n",
        "    - 데이터에서 각 변수마다 평균을 빼서, 모든 데이터가 중심 근처에 있도록 만들기\n",
        "    - 키 데이터가 [160, 170, 180] 이라면 평균 170을 뺴서\n",
        "    - [-10, 0, 10] 으로 만들기\n",
        "2. 공분산 행렬 만들기(Covariance Matrix)\n",
        "    - 각 변수들이 서로 얼마나 함께 변하는지 보기 위해 공분산 계산\n",
        "    - 키와 몸무게가 같이 증가하면 양의 공분산\n",
        "    - 키가 증가할수록 몸무게가 감소하면 음의 공분산\n",
        "    - 데이터가 어느 방향으로 퍼져 있는지 나타냄\n",
        "3. 고유값 분해\n",
        "    - 공분산 행렬을 고유값 분해를 하면 두 가지 값이 나옴\n",
        "    - 고유벡터(EigenVectors) : 데이터가 퍼져있는 방향\n",
        "    - 고유값(EigenValues) : 그 방향으로 얼마나 퍼져 있는지\n",
        "    - 고유벡터는 새로운 좌표축을 의미하고,\n",
        "    - 고유값은 그 축이 얼마나 중요한지를 나타냄\n",
        "4. 가장 중요한 방향만 선택(주성분 선택)\n",
        "    - 고유값이 큰 순서대로 고유벡터를 선택\n",
        "    - PC1 : 첫 번째 주성분 : 데이터의 분산이 가장 큰 방향\n",
        "    - PC2 : 두 번째 주성분 : PC1과 직교하면서 두 번쨰로 큰 분산을 가짐\n",
        "5. 데이터 변환(축소된 차원으로 표현)\n",
        "    - 원래 데이터를 선택한 주성분 축으로 투영해서, 낮은 차원의 데이터로 변환\n",
        "    - 핵심 정보는 유지되지만, 데이터는 단순화"
      ],
      "metadata": {
        "id": "DI7hQyyDWsqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "i4utdP8fY3dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "L_zxXbAQY37b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=37\n",
        ")"
      ],
      "metadata": {
        "id": "Do4KRti2ZOB3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링 전 원본 데이터 모델 평가\n",
        "rf_nopre = RandomForestClassifier(random_state=37)\n",
        "rf_nopre.fit(X_train, y_train)\n",
        "rf_nopre_predict = rf_nopre.predict(X_test)\n",
        "rf_nopre_acc = accuracy_score(y_test, rf_nopre_predict)\n",
        "print(f'정확도: {rf_nopre_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHAOZs8jZgG9",
        "outputId": "16bbafbe-a388-4c95-d5eb-97da27435b0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA(2D) + 모델(스케일링 없이)\n",
        "pca = PCA(n_components=2, random_state=37)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "lFcd85ymZ-U_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_pca = RandomForestClassifier(random_state=37)\n",
        "rf_pca.fit(X_train_pca, y_train)\n",
        "rf_pca_predict = rf_pca.predict(X_test_pca)\n",
        "rf_pca_acc = accuracy_score(y_test, rf_pca_predict)\n",
        "print(f'PCA 적용 정확도: {rf_pca_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU0jBqaKaQ2h",
        "outputId": "ff30e200-048d-4b40-fb74-55b6b684d237"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA 적용 정확도: 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA + 스케일링 후 모델\n",
        "ss = StandardScaler()\n",
        "X_train_s = ss.fit_transform(X_train)\n",
        "X_test_s = ss.transform(X_test)\n",
        "\n",
        "\n",
        "pca_scaled = PCA(n_components=2, random_state=37)\n",
        "X_train_pca_s = pca_scaled.fit_transform(X_train_s)\n",
        "X_test_pca_s = pca_scaled.transform(X_test_s)"
      ],
      "metadata": {
        "id": "icHYr7R2arIN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "rf_pca_ss = RandomForestClassifier(random_state=37)\n",
        "rf_pca_ss.fit(X_train_pca_s, y_train)\n",
        "rf_pca_ss_predict = rf_pca_ss.predict(X_test_pca_s)\n",
        "rf_pca_ss_acc = accuracy_score(y_test, rf_pca_ss_predict)\n",
        "print(f'스케일링한 차원축소 모델 학습 결과: {rf_pca_ss_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jImD0T7LbUG4",
        "outputId": "1c2583ee-d2d7-4535-cb11-18785e6ffd84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스케일링한 차원축소 모델 학습 결과: 0.833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링 전 PCA 설명 분산 비율\n",
        "# PC1, PC2\n",
        "# 4개의 피처를, 2개의 차원으로 줄인 결과\n",
        "# PC1 : 전체 데이터의 약 90%를 설명하고 있다\n",
        "# PC2 : 전체 데이터의 약 5%를 설명하고 있다\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2rl64wIfhKg",
        "outputId": "017af439-dd5a-45eb-cddf-d0f99782d79e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92023684, 0.05594972])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링 후 PCA 설명 분산 비율\n",
        "# 전보다 무게를 적절하게 분배하고 있음\n",
        "pca_scaled.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCX8L4bHfzB6",
        "outputId": "bfbe3fd4-c5b1-4b15-a00f-92524b959204"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72396729, 0.23240268])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "s9MieBiTgUlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 정확도 비교"
      ],
      "metadata": {
        "id": "k_BpcWMTguSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원본, 차원축소 모델, 차원축소와 스케일러 모델\n",
        "- 4차원 원본 데이터르 2차원으로 축소해도 모델이 높은 정확도를 유지\n",
        "- 2개의 주성분만으로도 데이터의 주요 판별 정보를 거의 보존했음을 의미\n",
        "- 스케일러와 차원축소를 적용한 모델은 정확도가 제일 낮게 나왔음\n",
        "- 스케일링으로 모든 특성을 동등한 분사 단위로 맞추고,\n",
        "- 그것을 다시 2차원으로 축소하니까\n",
        "- 그럼으로써 생기는 정보 손실이 발생해서 분류 성능이 저하될 수 있습니다."
      ],
      "metadata": {
        "id": "Kqr4DI9sgv3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 설명된 분산 비율"
      ],
      "metadata": {
        "id": "DtERAdnZhpti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 스케일링 전 PCA\n",
        "    - PC1 : 약 90%\n",
        "    - PC2 : 약 5%\n",
        "    - 하나의 툭성이 전체 분산을 거의 독점\n",
        "- 스케일링 후 PCA\n",
        "    - PC1 : 약 70%\n",
        "    - PC2 : 약 20%\n",
        "    = 주 성분이 상대적으로 고르게 데이터 변동성을 설명\n",
        "- 주성분들의 설명 비율이 총합 90% 이상이 되어야 의미가 있습니다."
      ],
      "metadata": {
        "id": "yjWbI__Ah3B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA 핵심 내용 정리"
      ],
      "metadata": {
        "id": "p_kfTz_JjNMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PCA는 선형 관계에서만 사용이 가능합니다.\n",
        "- 비선형도 할 수는 있다\n",
        "- 이미지, 텍스트 데이터에서 주로 사용"
      ],
      "metadata": {
        "id": "J98ZO1UTjOoJ"
      }
    }
  ]
}